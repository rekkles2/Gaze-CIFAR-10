# Gaze-Guided Learning: Avoiding Shortcut Bias in Visual Classification  

## Overview  
This repository provides an implementation of **Gaze-Guided Learning**, a method designed to reduce shortcut bias in visual classification using eye-tracking data. The model is based on **Vision Transformer (ViT)** and trained on the **Gaze-CIFAR-10** dataset.  

![Example Image](https://github.com/rekkles2/Gaze-CIFAR-10/blob/main/Figure/motivation.png) 

---

## Pretrained Model  
The **pretrained ViT model** can be downloaded from the following link:  
ğŸ”— [ViT Pretrained Model](https://drive.google.com/file/d/1FPUIYmZ4ooMbWByXUzBRNGLcrIYvNsxz/view?usp=drive_link)  

---

## Dataset  
The **Gaze-CIFAR-10** dataset can be downloaded from:  
ğŸ“‚ğŸ“‚ğŸ“‚ [**Gaze-CIFAR-10 Dataset**](https://drive.google.com/drive/folders/17zR9bIDWvb0FzSEgR2vXJIKo3w6wKDVB?usp=drive_link)  

![Example Image](https://github.com/rekkles2/Gaze-CIFAR-10/blob/main/Figure/EX.png)

---

## Training  
To **train** the model, use the following command:  
```bash
python train.py
```  

---

## Testing  
To **evaluate** the trained model, run:  
```bash
python predict1.py
```  

---

## Citation  
ğŸŸğŸ”ğŸ¥‚If you find this code or dataset useful in your research, please cite our work accordingly.  

---
